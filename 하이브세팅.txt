[Apache Hadoop ecosystem](https://www.oreilly.com/library/view/apache-hive-essentials/9781788995092/e846ea02-6894-45c9-983a-03875076bb5b.xhtml)
[Hive](https://datacookbook.kr/88)
데이터를 가상의 테이블 형태로 가진다
사용자는 sql qerry문을 사용할수있다
테이블은 사용자가 만든다
hdfs에 접근까지 설정하면 나머지는 하이브가 알아서

메타데이터 
서울시에서 10년의 기후데이터를 openData로 제공
컬럼이 10개고 데이터 10만개 첫번째는 어떤 커럼 두번째 ...
등등

데이터에 대한 인포메이션데이터를 메타데이터라 한다

메타데이터를 RDBMS로 hdfs의 메타데이터를 가진다
실제 정보는 하둡에 있다


sudo apt install mysql-server
sudo systemctl status mysql
하이브 사용시 문제는 없지만 mysql 기본설정을 해준다
sudo vi /etc/mysql/mysql.conf.d/mysqld.cnf
bind-address 때문에 외부 접속이 안된다 주석으로 막자
sudo systemctl restart mysql
sudo ufw allow mysql

sudo mysql 을 하면 루트로 들어가진다 
create database hive_db default character set utf8;
show databases;

하이브가 사용할 계정을 만들자
외부적소명(?)
 create user 'hive'@'%' identified by '1234';

내부적소명(?)
create user 'hive'@'localhost' identified by '1234';

유저만들고 권한주고
grant all privileges on hive_db.* to 'hive'@'%';
grant all privileges on hive_db.* to 'hive'@'localhost';
flush privileges; cache에 있는것을 보내주자
db 이름을 잘못 입력해서 권한을 주면 워크밴치에서 db가 보이지 않는다
잘 됐는지 확인 해보자
포트 포워딩
vmnetnat.conf 설정 후 NAT 서비스 재시작

mysql 계정추가
이름 - 계정이름 - 포트번호 - test connetion - password - ok
--mysql설치 완료
--hive를 설치해보자
apache hive - downloads - downloads a release now - https://dlcdn.apache.org/hive/ - hive-3.1.3 - apache-hive-3.1.3-bin.tar.gz(링크 주소 복사) - wget (붙혀넣기) - tar zxvf (이름)
-- 설정 시작
cd apache-hive-3.1.3-bin/conf
mv hive-env.sh.template hive-env.sh 이름 변경
vi hive-env.sh
vi ~/.bashrc
fg
HADOOP_HOME 찾아서 path 설정
cp hive-default.xml.template hive-site.xml 복사
vi hive-site.xml 
:24,6920d
아파치 더비 간단하게 쓸수 있는 one-user test를 할수 있는것이 존재
<configuration>
  <property>
    <name>hive.metastroe.local</name>
    <value>false</value>
  </property>
</configuration>

더비를 안쓰겠다는 뜻


MySQL Connector/J » 8.0.31 jar wget 한다
hdfs dfs -mkdir /tmp/user1
hdfs dfs -ls /tmp
hdfs dfs -chmod 777 /tmp/user1
hdfs dfs -ls /tmp
메타 스토어 초기화(아까 conf 파일 오타있으면 여기서 오류 남)
cd ../bin
ls -l
./schematool -initSchema -dbType mysql

SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/home/user1/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/home/user1/hadoop-3.2.4/share/hadoop/common/lib/slf4j-reload4j-1.7.35.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
        at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
        at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:536)
        at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:554)
        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:448)
        at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:5144)
        at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:5107)
        at org.apache.hive.beeline.HiveSchemaTool.<init>(HiveSchemaTool.java:96)
        at org.apache.hive.beeline.HiveSchemaTool.main(HiveSchemaTool.java:1473)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)

오류가 났는데 
하둡이 사용하는 guava 버전은 27
하이브가 사용하는 guava 버전은 19
하이브에 있는 guava를 지우고 하둡에 있는 guava를 가져온다
rm -f :~/apache-hive-3.1.3-bin/lib/guava*
~/apache-hive-3.1.3-bin/bin 이 위치에서
cp ~/hadoop-3.2.4/share/haddop/hdfs/lib/guava-27.0-jre.jar ../lib/
 ./schematool -initSchema -dbType mysql
ls
./hive
------하이브 입장
show databases;
하이브의 기본 데이터베이스 이름은 default라 함

----------여기서 잠깐
	2. HiveQL
	
	● 하이브는 HiveQL 이라는 SQL문과 매우 유사한 언어를 제공
	● 대부분의 기능은 SQL과 유사하지만 다음과 같은 차이점이 있음
	a. 하이브에서 사용하는 데이터가 HDFS에 저장되는데, HDFS가 한 번 저장한 파일은 수정할 수 없기 때문에 UPDATE와 DELETE를 사용할 수 없다.
	b. 같은 이유로 INSERT도 비어 있는 데이블에 입력하거나, 이미 입력된 데이터를 overwrite 하는 경우만 가능하다. 그래서 하이브는 INSERT OVERWRITE 라는 키워드를 사용한다.
	c. SQL은 어떠한 절에서도 서브쿼리를 사용할 수 있지만 HIveQL 은 FROM 절 에서만 서브 쿼리를 사용할 수 있다.
	d. SQL의 뷰는 업데이트할 수 있고, 구체화된 뷰 또는 인라인 뷰를 지원한다. 하지만 HiveQL의 뷰는 읽기 전용이며 인라인 뷰는 지원하지 않는다.
	e. SELECT 문을 사용할 때 HAVING 절을 사용할 수 없다.
저장 프로시저를 지원하지 않는다. 대신 맵리듀스 스크립트를 실행할 수 있다.
------
이전에 사용한 항공데이터를 이용하는데 하이브에 어떤 컬럼이 있는지 알려줘야한다
이를 하이브테이블을 만든다고 한다
만들어 보자
create table airline_delay(YEAR int, MONTH int, DAY_OF_MONTH int, DAY_OF_WEEK int, FL_DATE STRING, UNIQUE_CARRIER STRING,
TAIL_NUM STRING, FL_NUM INT, ORIGIN_AIRPORT_ID INT, ORIGIN STRING, ORIGIN_STATE_ABR STRING,
DEST_AIRPORT_ID INT, DEST STRING, DEST_STATE_ABR STRING, CRS_DEP_TIME STRING, DEP_TIME STRING, 
DEP_DELAY INT, DEP_DELAY_NEW INT, DEP_DEL15 INT, DEP_DELAY_GROUP INT, TAXI_OUT INT, WHEELS_OFF STRING,
WHEELS_ON STRING, TAXI_IN INT, CRS_ARR_TIME STRING, ARR_TIME STRING, ARR_DELAY INT, ARR_DELAY_NEW INT,
ARR_DEL15 INT, ARR_DELAY_GROUP INT, CANCELLED INT, CANCELLATION_CODE STRING, DIVERTED INT, CRS_ELAPSED_TIME INT,
ACTUAL_ELAPSED_TIME INT, AIR_TIME INT, FLIGHTS INT, DISTANCE INT, DISTANCE_GROUP INT, CARRIER_DELAY STRING,
WEATHER_DELAY STRING, NAS_DELAY STRING, SECURITY_DELAY STRING, LATE_AIRCRAFT_DELAY STRING)
partitioned by (delay_year int)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;
--- 테이블 주입 완료
show tables;
describe airline_delay;
--- 이제 테이터를 하이브에 load 하자
hdfs dfs -ls /user/hive/warehouse/airline_delay 저장 위치
load data inpath 'air-input/airOT199101.csv' into table airline_delay partition (delay_year=1991); 저장 
air-input에 있는 데이터를 airline_delay로 이동(move)시킨 것이다
select year, month, day_of_month, fl_date, unique_carrier, origin, dep_time, dep_delay from airline_delay where delay_year='1991' limit 20;


select year, month, count(*) as delay_count from airline_delay  where dep_delay > 0 group by year, month order by year, month;

연월별로 실제 지연 된것들의 평균도착지연시간
select year,month,sum(arr_delay) / count(*) as avg_delay_time from airline_delay where arr_delay > 0 group by year, month order by year, month;
왜 job이 2번 나눠서 되는 걸까

특정연도의 max(dep_delay)
select max(dep_delay) from airline_delay where year = '2012';

select fl_date, unique_carrier, origin, dep_time, dep_delay
from airline_delay a cross join (select max(dep_delay) as max_delay
from airline_delay where year = 2012) b
where a.dep_delay = b.max_delay and a.year=2012;

scp ./carriers.csv ubuntu:~/work

create table carrier_code(code string, description string)
row format delimited
fields terminated by ','
lines terminated by '\n'
stored as textfile;

 load data local inpath '/home/user1/work/carriers.csv' into table carrier_code;
하이브에서 직접 로컬에 접근해 자료를 가져간다

select a.fl_date, c.description, a.origin, a.dep_time, a.dep_delay
from airline_delay a cross join (select max(dep_delay) as max_delay
from airline_delay where year = 2012) b
join carrier_code c on a.unique_carrier = c.code
where a.dep_delay = b.max_delay
and a.year=2012;



alter talbe airline_delay drop artition(delay_year=1984)
